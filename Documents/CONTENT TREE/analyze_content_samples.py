#!/usr/bin/env python3
"""
Content Samples Analyzer
–ê–Ω–∞–ª—ñ–∑—É—î —ñ—Å–Ω—É—é—á—ñ —Ñ–∞–π–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç—É —Ç–∞ —Å—Ç–≤–æ—Ä—é—î —à–∞–±–ª–æ–Ω–∏ —Å—Ç–∏–ª—é –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–∏–ø—É.
"""

import os
import re
import yaml
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple

# –ö–æ—Ä–µ–Ω–µ–≤—ñ –ø–∞–ø–∫–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
CONTENT_ROOT = Path("/Users/marakinpoetry/Documents/CONTENT TREE/Content")
STAGES = ["Pre-Registration", "Trial", "Success_Client"]
CATEGORIES = ["Features", "Business_Types", "Competitors", "PM_Education", "Pains"]
CONTENT_TYPES = ["articles", "videos", "ads", "landings", "Case_Study", "SMM"]


def extract_frontmatter(content: str) -> Tuple[Dict, str]:
    """–í–∏—Ç—è–≥—É—î YAML frontmatter —Ç–∞ –æ—Å–Ω–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç."""
    pattern = r'^---\s*\n(.*?)\n---\s*\n(.*)$'
    match = re.match(pattern, content, re.DOTALL)

    if match:
        try:
            frontmatter = yaml.safe_load(match.group(1))
            body = match.group(2)
            return frontmatter, body
        except:
            return {}, content
    return {}, content


def analyze_structure(body: str) -> Dict:
    """–ê–Ω–∞–ª—ñ–∑—É—î —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–Ω—Ç–µ–Ω—Ç—É."""
    lines = body.strip().split('\n')

    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    h1_headers = re.findall(r'^# (.+)$', body, re.MULTILINE)
    h2_headers = re.findall(r'^## (.+)$', body, re.MULTILINE)
    h3_headers = re.findall(r'^### (.+)$', body, re.MULTILINE)

    # –†–∞—Ö—É—î–º–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏ (–±–ª–æ–∫–∏ —Ç–µ–∫—Å—Ç—É)
    paragraphs = [p for p in body.split('\n\n') if p.strip() and not p.strip().startswith('#')]

    # –†–∞—Ö—É—î–º–æ —Å–ª–æ–≤–∞
    words = len(body.split())

    # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Å–µ—Ä–µ–¥–Ω—é –¥–æ–≤–∂–∏–Ω—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
    avg_paragraph_length = sum(len(p.split()) for p in paragraphs) / len(paragraphs) if paragraphs else 0

    # –®—É–∫–∞—î–º–æ —Å–ø–∏—Å–∫–∏
    bullet_lists = len(re.findall(r'^\s*[-*]\s', body, re.MULTILINE))
    numbered_lists = len(re.findall(r'^\s*\d+\.\s', body, re.MULTILINE))

    # –®—É–∫–∞—î–º–æ —Ü–∏—Ç–∞—Ç–∏
    quotes = len(re.findall(r'^>\s', body, re.MULTILINE))

    # –®—É–∫–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
    links = len(re.findall(r'\[([^\]]+)\]\(([^)]+)\)', body))

    # –®—É–∫–∞—î–º–æ –≤–∏–∫–ª–∏–∫–∏ –¥–æ –¥—ñ—ó (CTA)
    cta_patterns = [
        r'—Å–ø—Ä–æ–±—É–π',
        r'—Å–ø—Ä–æ–±—É–≤–∞—Ç–∏',
        r'–∑–∞—Ä–µ—î—Å—Ç—Ä—É–π',
        r'–ø–æ—á–Ω–∏',
        r'–æ—Ç—Ä–∏–º–∞–π',
        r'–∑–∞–º–æ–≤–∏—Ç–∏',
        r'try\s',
        r'start\s',
        r'get\s',
        r'register',
        r'–ø–æ–ø—Ä–æ–±—É–π'
    ]
    cta_count = sum(len(re.findall(pattern, body, re.IGNORECASE)) for pattern in cta_patterns)

    return {
        "h1_count": len(h1_headers),
        "h2_count": len(h2_headers),
        "h3_count": len(h3_headers),
        "h1_headers": h1_headers[:3],  # –ü–µ—Ä—à—ñ 3 –¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—É
        "h2_headers": h2_headers[:5],  # –ü–µ—Ä—à—ñ 5 –¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—É
        "paragraph_count": len(paragraphs),
        "word_count": words,
        "avg_paragraph_length": round(avg_paragraph_length),
        "bullet_lists": bullet_lists,
        "numbered_lists": numbered_lists,
        "quotes": quotes,
        "links": links,
        "cta_count": cta_count
    }


def analyze_tone_and_style(body: str) -> Dict:
    """–ê–Ω–∞–ª—ñ–∑—É—î —Ç–æ–Ω —Ç–∞ —Å—Ç–∏–ª—å –Ω–∞–ø–∏—Å–∞–Ω–Ω—è."""

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å (–≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è "–í–∏" vs "—Ç–∏")
    formal_pronouns = len(re.findall(r'\b(–í–∏|–í–∞—Å|–í–∞–º|–í–∞–º–∏|You|you)\b', body))
    informal_pronouns = len(re.findall(r'\b(—Ç–∏|—Ç–æ–±—ñ|—Ç–µ–±–µ|—Ç–æ–±–æ—é|—Ç—ã|—Ç–µ–±—è|—Ç–µ–±–µ)\b', body))

    # –ï–º–æ—Ü—ñ–π–Ω—ñ—Å—Ç—å (–∑–Ω–∞–∫–∏ –æ–∫–ª–∏–∫—É)
    exclamation_marks = body.count('!')

    # –ü–∏—Ç–∞–ª—å–Ω—ñ—Å—Ç—å
    question_marks = body.count('?')

    # –¢–µ—Ö–Ω—ñ—á–Ω—ñ—Å—Ç—å (–Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —Ç–µ—Ä–º—ñ–Ω—ñ–≤)
    technical_terms = [
        'API', '—ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è', '—Ñ—É–Ω–∫—Ü—ñ—è', '–Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è', '–∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è',
        'integration', 'function', 'configuration', 'settings',
        '–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è', '—Ñ—É–Ω–∫—Ü–∏—è', '–Ω–∞—Å—Ç—Ä–æ–π–∫–∏'
    ]
    technical_count = sum(body.lower().count(term.lower()) for term in technical_terms)

    # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ü–∏—Ñ—Ä —Ç–∞ –º–µ—Ç—Ä–∏–∫
    numbers = len(re.findall(r'\b\d+%', body))

    # –î–æ–≤–∂–∏–Ω–∞ —Ä–µ—á–µ–Ω—å (—Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å)
    sentences = re.split(r'[.!?]+', body)
    avg_sentence_length = sum(len(s.split()) for s in sentences if s.strip()) / len([s for s in sentences if s.strip()])

    tone = "formal" if formal_pronouns > informal_pronouns else "informal"
    emotion_level = "high" if exclamation_marks > 5 else "medium" if exclamation_marks > 2 else "low"
    technical_level = "high" if technical_count > 10 else "medium" if technical_count > 5 else "low"

    return {
        "tone": tone,
        "emotion_level": emotion_level,
        "technical_level": technical_level,
        "exclamation_marks": exclamation_marks,
        "question_marks": question_marks,
        "metrics_count": numbers,
        "avg_sentence_length": round(avg_sentence_length, 1)
    }


def analyze_file(file_path: Path) -> Dict:
    """–ê–Ω–∞–ª—ñ–∑—É—î –æ–¥–∏–Ω —Ñ–∞–π–ª."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        frontmatter, body = extract_frontmatter(content)
        structure = analyze_structure(body)
        style = analyze_tone_and_style(body)

        return {
            "file_path": str(file_path.relative_to(CONTENT_ROOT)),
            "frontmatter": frontmatter,
            "structure": structure,
            "style": style,
            "success": True
        }
    except Exception as e:
        return {
            "file_path": str(file_path),
            "error": str(e),
            "success": False
        }


def get_sample_files(category_path: Path, content_type: str, limit: int = 2) -> List[Path]:
    """–û—Ç—Ä–∏–º—É—î –ø—Ä–∏–∫–ª–∞–¥–∏ —Ñ–∞–π–ª—ñ–≤ –∑ –ø–∞–ø–∫–∏."""
    content_type_path = category_path / content_type

    if not content_type_path.exists():
        return []

    # –®—É–∫–∞—î–º–æ .md —Ñ–∞–π–ª–∏
    md_files = list(content_type_path.glob("*.md"))

    # –Ø–∫—â–æ —Ñ–∞–π–ª—ñ–≤ –±—ñ–ª—å—à–µ limit, –±–µ—Ä–µ–º–æ –Ω–∞–π–Ω–æ–≤—ñ—à—ñ
    if len(md_files) > limit:
        md_files = sorted(md_files, key=lambda x: x.stat().st_mtime, reverse=True)[:limit]

    return md_files


def aggregate_analysis(analyses: List[Dict]) -> Dict:
    """–ê–≥—Ä–µ–≥—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑—É –∫—ñ–ª—å–∫–æ—Ö —Ñ–∞–π–ª—ñ–≤."""
    if not analyses:
        return {}

    # –°–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
    structure_keys = ["h1_count", "h2_count", "h3_count", "paragraph_count",
                     "word_count", "avg_paragraph_length", "bullet_lists",
                     "numbered_lists", "quotes", "links", "cta_count"]

    aggregated = {
        "sample_count": len(analyses),
        "structure": {},
        "style": {}
    }

    # –ê–≥—Ä–µ–≥–∞—Ü—ñ—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
    for key in structure_keys:
        values = [a["structure"][key] for a in analyses if a.get("success")]
        if values:
            aggregated["structure"][key] = {
                "min": min(values),
                "max": max(values),
                "avg": round(sum(values) / len(values), 1)
            }

    # –ó–±–∏—Ä–∞—î–º–æ –ø—Ä–∏–∫–ª–∞–¥–∏ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
    all_h2 = []
    for a in analyses:
        if a.get("success"):
            all_h2.extend(a["structure"].get("h2_headers", []))
    aggregated["structure"]["h2_examples"] = list(set(all_h2))[:10]

    # –ê–≥—Ä–µ–≥–∞—Ü—ñ—è —Å—Ç–∏–ª—é
    tones = [a["style"]["tone"] for a in analyses if a.get("success")]
    emotions = [a["style"]["emotion_level"] for a in analyses if a.get("success")]
    technical = [a["style"]["technical_level"] for a in analyses if a.get("success")]

    aggregated["style"] = {
        "dominant_tone": max(set(tones), key=tones.count) if tones else "unknown",
        "dominant_emotion": max(set(emotions), key=emotions.count) if emotions else "unknown",
        "dominant_technical": max(set(technical), key=technical.count) if technical else "unknown",
        "avg_sentence_length": round(sum(a["style"]["avg_sentence_length"] for a in analyses if a.get("success")) / len(analyses), 1)
    }

    # –ó–±–∏—Ä–∞—î–º–æ –ø—Ä–∏–∫–ª–∞–¥–∏ frontmatter
    frontmatters = [a["frontmatter"] for a in analyses if a.get("success") and a.get("frontmatter")]
    if frontmatters:
        aggregated["frontmatter_example"] = frontmatters[0]

    return aggregated


def scan_content_tree():
    """–°–∫–∞–Ω—É—î –≤—Å—é Content Tree —Ç–∞ –∞–Ω–∞–ª—ñ–∑—É—î –∫–æ–Ω—Ç–µ–Ω—Ç."""
    results = defaultdict(lambda: defaultdict(list))

    print("üîç –°–∫–∞–Ω—É—é Content Tree...")
    print()

    for stage in STAGES:
        stage_path = CONTENT_ROOT / stage
        if not stage_path.exists():
            continue

        print(f"üìÇ Stage: {stage}")

        for category in CATEGORIES:
            category_path = stage_path / category
            if not category_path.exists():
                continue

            print(f"  üìÅ Category: {category}")

            # –®—É–∫–∞—î–º–æ –≤—Å—ñ –ø—ñ–¥–ø–∞–ø–∫–∏ (—Ç–µ–º–∏)
            for topic_path in category_path.iterdir():
                if not topic_path.is_dir():
                    continue

                topic_name = topic_path.name

                # –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–∏–ø—É –∫–æ–Ω—Ç–µ–Ω—Ç—É
                for content_type in CONTENT_TYPES:
                    sample_files = get_sample_files(topic_path, content_type, limit=2)

                    if not sample_files:
                        continue

                    print(f"    üìÑ {content_type}: {len(sample_files)} samples from {topic_name}")

                    analyses = []
                    for file_path in sample_files:
                        analysis = analyze_file(file_path)
                        if analysis["success"]:
                            analyses.append(analysis)

                    if analyses:
                        key = f"{stage}_{category}_{content_type}"
                        results[key]["analyses"] = analyses
                        results[key]["aggregated"] = aggregate_analysis(analyses)
                        results[key]["stage"] = stage
                        results[key]["category"] = category
                        results[key]["content_type"] = content_type

    print()
    print(f"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ {len(results)} –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π –∫–æ–Ω—Ç–µ–Ω—Ç—É")
    return dict(results)


def generate_template_markdown(results: Dict) -> str:
    """–ì–µ–Ω–µ—Ä—É—î CONTENT_TEMPLATES.md –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞–Ω–∞–ª—ñ–∑—É."""

    md = """# Content Templates & Style Guide
–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞–Ω–∞–ª—ñ–∑—É —ñ—Å–Ω—É—é—á–∏—Ö —Ñ–∞–π–ª—ñ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç—É.

–¶–µ–π —Ñ–∞–π–ª –º—ñ—Å—Ç–∏—Ç—å —à–∞–±–ª–æ–Ω–∏ —Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–Ω—Ç–µ–Ω—Ç—É, —Å—Ö–æ–∂–æ–≥–æ –Ω–∞ —ñ—Å–Ω—É—é—á—ñ –ø—Ä–∏–∫–ª–∞–¥–∏.

---

"""

    # –ì—Ä—É–ø—É—î–º–æ –∑–∞ —Ç–∏–ø–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç—É
    by_content_type = defaultdict(list)
    for key, data in results.items():
        content_type = data["content_type"]
        by_content_type[content_type].append((key, data))

    for content_type, items in sorted(by_content_type.items()):
        md += f"\n## {content_type.upper()}\n\n"

        for key, data in items:
            stage = data["stage"]
            category = data["category"]
            agg = data["aggregated"]

            md += f"### {stage} / {category}\n\n"
            md += f"**–ë–∞–∑—É—î—Ç—å—Å—è –Ω–∞ {agg['sample_count']} —Ä–µ–∞–ª—å–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥–∞—Ö**\n\n"

            # –°—Ç—Ä—É–∫—Ç—É—Ä–∞
            md += "#### üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞\n\n"
            struct = agg["structure"]

            md += f"- **–î–æ–≤–∂–∏–Ω–∞**: {struct.get('word_count', {}).get('min', 0)} - {struct.get('word_count', {}).get('max', 0)} —Å–ª—ñ–≤ "
            md += f"(—Å–µ—Ä–µ–¥–Ω—î: {struct.get('word_count', {}).get('avg', 0)})\n"

            md += f"- **–ó–∞–≥–æ–ª–æ–≤–∫–∏ H2**: {struct.get('h2_count', {}).get('min', 0)} - {struct.get('h2_count', {}).get('max', 0)} "
            md += f"(—Å–µ—Ä–µ–¥–Ω—î: {struct.get('h2_count', {}).get('avg', 0)})\n"

            md += f"- **–ü–∞—Ä–∞–≥—Ä–∞—Ñ—ñ–≤**: {struct.get('paragraph_count', {}).get('min', 0)} - {struct.get('paragraph_count', {}).get('max', 0)}\n"

            md += f"- **–°–µ—Ä–µ–¥–Ω—è –¥–æ–≤–∂–∏–Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞**: ~{struct.get('avg_paragraph_length', {}).get('avg', 0)} —Å–ª—ñ–≤\n"

            if struct.get('bullet_lists', {}).get('avg', 0) > 0:
                md += f"- **–ú–∞—Ä–∫–æ–≤—ñ —Å–ø–∏—Å–∫–∏**: –ø—Ä–∏—Å—É—Ç–Ω—ñ (~{struct.get('bullet_lists', {}).get('avg', 0)} –ø—É–Ω–∫—Ç—ñ–≤)\n"

            if struct.get('cta_count', {}).get('avg', 0) > 0:
                md += f"- **CTA (–∑–∞–∫–ª–∏–∫–∏ –¥–æ –¥—ñ—ó)**: ~{struct.get('cta_count', {}).get('avg', 0)} —Ä–∞–∑—ñ–≤\n"

            # –ü—Ä–∏–∫–ª–∞–¥–∏ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ H2
            if struct.get("h2_examples"):
                md += "\n**–¢–∏–ø–æ–≤—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏ H2:**\n"
                for h2 in struct["h2_examples"][:5]:
                    md += f"- {h2}\n"

            md += "\n"

            # –°—Ç–∏–ª—å
            md += "#### üé® –°—Ç–∏–ª—å —Ç–∞ —Ç–æ–Ω\n\n"
            style = agg["style"]

            tone_map = {
                "formal": "–§–æ—Ä–º–∞–ª—å–Ω–∏–π (–í–∏)",
                "informal": "–ù–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–∏–π (—Ç–∏)"
            }
            md += f"- **–¢–æ–Ω**: {tone_map.get(style['dominant_tone'], style['dominant_tone'])}\n"

            emotion_map = {
                "high": "–í–∏—Å–æ–∫–∏–π (–±–∞–≥–∞—Ç–æ –µ–º–æ—Ü—ñ–π, –∑–Ω–∞–∫—ñ–≤ –æ–∫–ª–∏–∫—É)",
                "medium": "–°–µ—Ä–µ–¥–Ω—ñ–π (–ø–æ–º—ñ—Ä–Ω–∞ –µ–º–æ—Ü—ñ–π–Ω—ñ—Å—Ç—å)",
                "low": "–ù–∏–∑—å–∫–∏–π (—Å—Ç—Ä–∏–º–∞–Ω–∏–π, —Ñ–∞–∫—Ç–æ–ª–æ–≥—ñ—á–Ω–∏–π)"
            }
            md += f"- **–ï–º–æ—Ü—ñ–π–Ω—ñ—Å—Ç—å**: {emotion_map.get(style['dominant_emotion'], style['dominant_emotion'])}\n"

            tech_map = {
                "high": "–í–∏—Å–æ–∫–∏–π (–±–∞–≥–∞—Ç–æ —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —Ç–µ—Ä–º—ñ–Ω—ñ–≤)",
                "medium": "–°–µ—Ä–µ–¥–Ω—ñ–π (–±–∞–ª–∞–Ω—Å —Ç–µ—Ö–Ω—ñ—á–Ω–æ—Å—Ç—ñ —Ç–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ)",
                "low": "–ù–∏–∑—å–∫–∏–π (–¥–æ—Å—Ç—É–ø–Ω–∞ –º–æ–≤–∞ –¥–ª—è —à–∏—Ä–æ–∫–æ—ó –∞—É–¥–∏—Ç–æ—Ä—ñ—ó)"
            }
            md += f"- **–¢–µ—Ö–Ω—ñ—á–Ω—ñ—Å—Ç—å**: {tech_map.get(style['dominant_technical'], style['dominant_technical'])}\n"

            md += f"- **–°–µ—Ä–µ–¥–Ω—è –¥–æ–≤–∂–∏–Ω–∞ —Ä–µ—á–µ–Ω–Ω—è**: {style['avg_sentence_length']} —Å–ª—ñ–≤\n"

            md += "\n"

            # Frontmatter –ø—Ä–∏–∫–ª–∞–¥
            if agg.get("frontmatter_example"):
                md += "#### üìù –ü—Ä–∏–∫–ª–∞–¥ YAML Frontmatter\n\n"
                md += "```yaml\n"
                md += yaml.dump(agg["frontmatter_example"], allow_unicode=True, default_flow_style=False)
                md += "```\n\n"

            md += "---\n\n"

    # –î–æ–¥–∞—î–º–æ –∑–∞–≥–∞–ª—å–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
    md += """
## üéØ –ó–∞–≥–∞–ª—å–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó

### –ü—Ä–∏–Ω—Ü–∏–ø–∏ —Å—Ö–æ–∂–æ—Å—Ç—ñ

1. **–ß–∏—Ç–∞–π –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—î—é**: –ó–∞–≤–∂–¥–∏ —á–∏—Ç–∞–π 1-2 –ø—Ä–∏–∫–ª–∞–¥–∏ –∑ —Ü—ñ–ª—å–æ–≤–æ—ó –ø–∞–ø–∫–∏ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—î—é
2. **–ö–æ–ø—ñ—é–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É**: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ç–∞–∫—É —Å–∞–º—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ —Ç–∞ —Ä–æ–∑–¥—ñ–ª—ñ–≤
3. **–í—ñ–¥—Ç–≤–æ—Ä—é–π —Ç–æ–Ω**: –ó–±–µ—Ä–µ–∂–∏ —Ñ–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å/–Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å, –µ–º–æ—Ü—ñ–π–Ω—ñ—Å—Ç—å, —Ç–µ—Ö–Ω—ñ—á–Ω—ñ—Å—Ç—å
4. **–î–æ—Ç—Ä–∏–º—É–π—Å—è –¥–æ–≤–∂–∏–Ω–∏**: –ì–µ–Ω–µ—Ä—É–π —Ç–µ–∫—Å—Ç –ø—Ä–∏–±–ª–∏–∑–Ω–æ —Ç–∞–∫–æ—ó —Å–∞–º–æ—ó –¥–æ–≤–∂–∏–Ω–∏ —è–∫ –ø—Ä–∏–∫–ª–∞–¥–∏
5. **–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ä–µ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ**: –¢—ñ–ª—å–∫–∏ –∑ Knowledge Base, –Ω—ñ–∫–æ–ª–∏ –Ω–µ –≤–∏–≥–∞–¥—É–π –º–µ—Ç—Ä–∏–∫–∏

### Smart Mix Formula

```
–ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç = PROMPTS.md (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞) + –†–µ–∞–ª—å–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ (—Å—Ç–∏–ª—å) + Knowledge Base (–¥–∞–Ω—ñ)
```

### –Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è –¥–ª—è Claude

–ü—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–Ω—Ç–µ–Ω—Ç—É:

1. –ü—Ä–æ—á–∏—Ç–∞–π –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π –ø—Ä–æ–º–ø—Ç –∑ PROMPTS.md
2. –ü—Ä–æ—á–∏—Ç–∞–π 1-2 —Ä–µ–∞–ª—å–Ω—ñ —Ñ–∞–π–ª–∏ –∑ —Ü—ñ–ª—å–æ–≤–æ—ó –ø–∞–ø–∫–∏
3. –í–∏—Ç—è–≥–Ω–∏ –∑ –Ω–∏—Ö:
   - –°—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–æ–∑–¥—ñ–ª—ñ–≤ (H1, H2, H3)
   - –¢–æ–Ω –∑–≤–µ—Ä—Ç–∞–Ω–Ω—è (–í–∏ —á–∏ —Ç–∏)
   - –†—ñ–≤–µ–Ω—å –µ–º–æ—Ü—ñ–π–Ω–æ—Å—Ç—ñ
   - –†—ñ–≤–µ–Ω—å —Ç–µ—Ö–Ω—ñ—á–Ω–æ—Å—Ç—ñ
   - –¢–∏–ø–æ–≤—É –¥–æ–≤–∂–∏–Ω—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ñ–≤
4. –ó–≥–µ–Ω–µ—Ä—É–π –∫–æ–Ω—Ç–µ–Ω—Ç —è–∫–∏–π –≤–∏–≥–ª—è–¥–∞—î –¢–ê–ö –°–ê–ú–û —è–∫ –ø—Ä–∏–∫–ª–∞–¥–∏, –∞–ª–µ –º—ñ—Å—Ç–∏—Ç—å –Ω–æ–≤—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é
5. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò —Ä–µ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –∑ Knowledge Base

### –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫–æ—Å—Ç—ñ

–ü—ñ—Å–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–≤–∏–Ω–µ–Ω:
- ‚úÖ –ú–∞—Ç–∏ —Å—Ö–æ–∂—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏
- ‚úÖ –ú—ñ—Å—Ç–∏—Ç–∏ —Ä–µ–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –∑ KB
- ‚úÖ –ú–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ hierarchical_tags
- ‚úÖ –í—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ —Ç–æ–Ω—É —Ç–∞ —Å—Ç–∏–ª—é –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
- ‚úÖ –ë—É—Ç–∏ —Ç–∞–∫–æ—ó —Å–∞–º–æ—ó –¥–æ–≤–∂–∏–Ω–∏ (¬±20%)
"""

    return md


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è."""
    print("=" * 60)
    print("Content Samples Analyzer")
    print("=" * 60)
    print()

    # –°–∫–∞–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ –∞–Ω–∞–ª—ñ–∑
    results = scan_content_tree()

    # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —à–∞–±–ª–æ–Ω—ñ–≤
    print()
    print("üìù –ì–µ–Ω–µ—Ä—É—é CONTENT_TEMPLATES.md...")
    template_md = generate_template_markdown(results)

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    output_path = Path("/Users/marakinpoetry/Documents/CONTENT TREE/docs/CONTENT_TEMPLATES.md")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(template_md)

    print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {output_path}")
    print()
    print("=" * 60)
    print("–ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ CONTENT_TEMPLATES.md")
    print("–¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ö–æ–∂–æ–≥–æ –Ω–∞ —ñ—Å–Ω—É—é—á—ñ –ø—Ä–∏–∫–ª–∞–¥–∏.")
    print("=" * 60)


if __name__ == "__main__":
    main()
